This document consists of a lot of different write ups copy and pasted together
and it has not been thoroughly proofread. This document is intended for historical
purposes but I don't expect it to read coherently like the README does.

##################################################################################

ALGORITHM IMPROVEMENT IDEAS!

These notes were living in TODO but it got a bit sloppy. I spent a while
writing down my thoughts and a little math for these changes so it would be
nice to keep those notes documented somewhere. Although I'm not really planning
on documenting the old code anywhere other than the git history.

Each one of these changes constitutes at least a 0.0.1 version number increase.
These changes will not effect any of the API and therefore any of the tests. So I 
will use pytest with our 10 existing tests to time out how much
these improvements speed up testing.

These changes are in linear order from the time of writing them so all test times carry
forward but there may be many more tests in the future.

BEFORE TIMES:
real    0m0.848s
user    0m0.747s
sys     0m0.102s

Modify IceBrakes algo to only read each file once. You still build two
dicts for CONSTANTS and ALL_VARS but you do it in one pass.

This will be uglier code in some ways with a bit more nesting and a bit less
DRY potential. 

Our bigO time improvement looks like this:  O(Nᴹ + Nᴹ⁺¹)  ->  O(Nᴹ⁺¹)
Where N is the length of the file and M is the amount of checks I do per line
and the +1 represents the single extra check for #$ at the end of each line.

I'm having real trouble parsing this math because it casts a pretty wide net.
For smaller numbers we are looking at close to 100% speed up or double speed. Hopefully
that works out for my tests but as the size of N and M grow this improvement matters less.

Either way because the tests are very small the act of reading the file may actually be a
huge time sink and I think that we might see some huge improvements in the tests. Which I'm
now realizing is a classic trap to make one think their program is faster than it is.

Still I want my tests to run faster so I can do more of everything else.

The API change will look like this:
function get_names_from_file() will not be called twice anymore, instead we will
both dicts in one pass. This means the boolean check goes and the nifty logic with
target='' goes too. Then this method can be called wherever the two dicts are needed.

This one change will actually DRY this up a fair amount.

This broken test 0011 for a minute for reasons documented in the test file. Now we test
the new behavior instead. Which is more thorough anyways.

AFTER TIMES:
real    0m0.875s
user    0m0.755s
sys     0m0.122s

It's worth noting that currently my tests are very very short files which has been 
practical to maintain even though it's not realistic. I should grab some existing large
open source python code, and add my silly marks to it and use it for benchmarking my
program instead of the unit tests. I should have longer tests too.

It's possible my current unit tests run at a pretty linear near O(1) already but that doesn't 
invalidate my improvements or my math. 

I added a new benchmark that is 16k lines long and declares #$ on every single line
giving us our maximum possible runtime. We are not currently concerned with accurately
linting this file but we do want to track algo time. Now even though I tried to make
this fairly slow it still only takes 5.7 seconds. But I think that will be enough to
mark any further improvements.

BEFORE TIMES:
real    0m7.806s
user    0m5.724s
sys     0m0.782s

Modify IceBrakes algo to ignore names that aren't in the constants
dict. This will skip a large amount of checks, and when more tests
are run things will be way faster. 

Use the names as keys and the indexes as values. This is what I should have done
the whole time and it will be a time improvement on it's own as there are less total dict keys
to loop over.

(TIME ABOVE CHANGES BEFORE CONTINUING)

Next; loop through the keys only and do all_vars.get(constants_key) to determine values.
This is clever because it removes an entire for loop and python's get method is very fast.

O(Nᴹ)  ->  O(N)

I will have to review my code closely to figure out the real max time. It's written as a nested for
loop wright now which is bad. I need to loop over every line of the file exactly 1 for the purpose
of writing an output results.

O(Nᴹ)  ->  O(N)
O(Nᴹ + Nᴹ⁺¹)  ->  O(Nᴹ⁺¹)

This change was implemented in a scratchpad and move over to icebrakes.py without too much
trouble. This testing suite has been invaluable so far.

AFTER TIMES:
real    0m4.427s
user    0m1.233s
sys     0m0.657s

Nice! We went from 5.7 to 1.2 seconds and scored a 4.76x speed improvement. Now that both
algo changes are done, I need to focus on the white space parsing.

#######################################################################################

Below is all the notes I wrote about the algorithm changes I made to add scope awareness,
I figured I would leave it documented here since it was pretty successful and I already typed
it all out. There were some minor tweaks I had to make get this process working. See the git
changelog from version 0.0.7 to 0.1.1 to see all code changes.

Eventually the functions of this file may get pulled into icebrakes.py, but first I want to
build my algorithms without breaking any of the unit tests.

My original thought process was naive and I assumed I needed to track every white space
change but since I only care about namespaces it's actually a much simpler problem, and
the main engines are already written.

Below I lay out how I will use white_space_parse() from this file along with get_names_from_file()
and paren_parse() from icebrakes.py to solve for namespaces.

Here's the deal with scopes, only the keywords 'def', 'class', 'async def' can
trigger a change in namespace. We don't need to track any other keywords. Words like
'for' and 'if' won't effect this is any way. My original line of thinking considered ternary 
if statements as an edge case but they are not even on my radar anymore.

So what do we really need? A different set of name dicts (constants/all_vars) for each sub 
namespace which will be determined by one of the above keywords and a corresponding white 
space change. We track when the white space goes back to it's original level, and that kills 
the sub namespace.

The good news is we don't need to parse lines more than once, but we need to juggle a lot
more dictionaries. So I need to figure out an api or tool or recursion that allows us to
sub name space for a while and then return out. 

We also either need to parse each set of sub dicts as we are leaving the scope or we need a
dict of all total dicts where the keys are related to the scope that we parse once after reading 
the file. The keys can't be indentation level because two different functions can have the same 
indent level and not be in each other's scopes.

So the name of each function can kind of work as a key but they would need to be sub keyed to their
outer scope, and then everything at the top would be named 'root'.

Here is a python code example.

def foo():
    def bar():
        pass

def bar():
    def foo():
        pass
        
We want to parse than into an object that looks like this.

root.foo
root.foo.bar
root.bar
root.bar.foo

I imagine that is a solved problem in python but it's one I will have to research.

It might be worth implementing some OO for the name dicts. Some struct that contains two
dicts. It will make passing things around a bit easier.

Because I already have paren_parse, I can actually use it to set a flag whenever a new
scope is found, then we unset the flag later by tracking white space. This would basically
mean checking for the sub scope flag (or int?) every pass and then carefully tracking white
space when it is set. This will have a huge time deficit but I always knew that.

The reason I think we need an int instead of a bool for a flag is to track how many sub scopes
deep we are. So 0 means root and 1 means one function def in, etc. Then every time we leave a scope,
we subtract one, and once the number is 0 we're not in a scope anymore. This will work with a nice
'if flag:' check because 0 is already falsey.

In get_names_from_file() is where I should track white space and check flags. I need to count 
the whitespace before stripping it, and move left/right accordingly. 

Things are structured and functional enough now that I think I can achieve this change with
only one or two additional functions. My next step will be to write out what those functions
might look like, and then I will try building that engine. When this step is complete most of 
the rest of the unit tests can be turned on because a large amount of them relate to white space.

I will also need a new major feature to work towards (loops) but I'm sure something will come up.

Upon further thinking I have realized that by creating unique keys I won't actually need the
nested dict design pattern. Instead I will make a Naming() class that takes a name, and a
States() object and builds unique name keys. Then anytime two keys match exactly we have
a scope problem at the earliest place in the keys that match. I'm going to build the changes
to both classes, and then build a name parsing engine and then I barely have to change any
of the rest of the code.


Implement these tools in the following ways

Get indentation level at beginning of parse.
base_indent: int = white_space_parse()

Get the space at beginning of line
spaces: int = len('line') - len('line'.rsplit())

For every line parsed run these two lines
indent: int = spaces // base_indent
States.update_indent(indent)

When paren_parse returns positive add that name to states.names_in_scope
with the current indentation level.

Use name_gen() to get a real name only after 'if name:' checks
Use name_split() only when putting a name in message.

With those changes the test suite shouldn't be broken. Give it a try.

AFTER TIMES:
real    0m1.834s
user    0m1.713s
sys     0m0.114s


*Directory support (v0.1.5)

    IceBrakes should run with either a file or a dir of files as input. I will add
    a bool to get_names_from_file() that tracks when a dir is being parsed. When
    in dir_more we return an exit code instead of exiting with that code. Here is an
    example of the code.

    def get_names_from_file(file: str, dir_mode: bool=False) -> int:
        exit_code: int = 0
        if dir_mode:
            return exit_code
        else:
            exit(exit_code)

    Then the dir function just collects all the exit codes and prints them all
    for now. This won't break a single unit test or the API.

    My original plan for this was a bit convoluted but os.walk saved the day and I
    don't need to document it further because the code is very self explanatory.

AFTER TIMES:
real    0m10.014s
user    0m2.706s
sys     0m1.421s

A little bit of a regression but I added a lot of complexity in the last few updates
so that makes sense.


* Loop awareness (v0.1.7)

    This is not too different from parsing def/class etc. Basically the rule is just
    "You can never assign an immutable in the body of a loop." So we only care about
    loops when building the constants dict. This error is different because it's an 
    illegal use of "#$" all together. We will give the message "Constant assigned 
    inside of a loop on line ##, consider assigning outside of the loop instead."
    
    We also need a way to track whether or not we're in the body of a loop,
    and I'm not sure of the fastest way to do that yet. The indentation level is already
    being tracked so all we need is a like a "in loop" bool.

    "break" statements are a weird edge case here because I'm not executing or following the
    logic of the code, and I really can't think of a way a break statement would effect
    immutable vars. Instead I'll be concerned only with whitespace for exiting loops. 

    We have to track every single loop in order to know if "#$" is being used inside a loop
    so there isn't really a shortcut here. 

    I used the sample API inside of the States class that names_in_scope uses and I added
    an instance variable called loops_in_scope. This change was achieved in exactly 12 new 
    lines of code, and I guessed it would take 4-8 lines. Close enough!

AFTER TIMES:
real    0m0.621s
user    0m0.298s
sys     0m0.059s

This is weird, I added complexity and got a near 10x time improvement. I reran v0.1.5
and got the same times as above. I have no clue what caused such a speedup, but all the 
tests are still passing so I'll take it.

v0.1.5 TIMES
real    0m9.546s
user    0m2.582s
sys     0m1.385s

I just figured out what caused the speedup and I'm leaving it as an exercise for the reader.
But it's worth mentioning that my benchmark, by design, is not a real world example and so 
it sees dramatic speed changes.


* String Parser (v0.1.8)
    
    For single line strings we only need to worry about equal line parse
    and I think it might be abstractly simple in that we only care if ' or "
    occur before the equal sign.

    'word' = 5   # illegal

    This going to be implemented in the same way I originally approached skipping
    parenthesis before I wrote paren_parser (which could use a new name).

    I'm also going to add that feature back in to avoid lines like this

    foo(bar=55)
    since that should be allowed. 

    These changes will take place exclusively in equal_sign_parse() and I
    will also change the order of equal_sign_parse() and paren_parse() assuming
    that doesn't break any tests.

    I managed to implement these changes in four lines of code. Now onto multi line strings.
    My best guess is that multiline strings will take 6-12 lines of code as it involves a 
    fair bit more state.

AFTER TIMES:
real    0m0.617s
user    0m0.326s
sys     0m0.031s


* Multiline Comment Engine (v0.2.0)

# More thorough writeup
#### MULTILINE NOTES ####

At the top of the get_names_from_file() loop for every line of the file we
will check for triple quotes to either enter or exit a multiline comment. If
an entrance is found we have to check the rest of the line for an exit. So
this will require a separate function. We'll have a variable in States() that
allows us to skip the rest of the line checks until the multi line string
is exited.

There can only be one multiline string at a time so we only need to track
whether it was single or double quotes that entered the string and once
a string is exited we need to continue tracking.

It might be best to use regex to get every occurrence of """ from the line
and then do some logic based on the tree of """s. We need to track groups
of three quotes at a time, and track the index of the third quote. So that
a pattern line """" doesn't appear as an entrance and an exit. (regex was
quickly abandoned because it does not fit the problem at all.)

Also """" "" is not a valid exit either. It needs to be another group of
three quotes like """""" or """ """

If there are characters before the string starts then we do need to parse
the line. So this is mostly for tracking when entire sections of a file
are not to be parsed.


AFTER TIMES:
real    0m0.522s
user    0m0.301s
sys     0m0.040s

    No noticeable difference again but that's good for how complex this feature was. All told
    this feature clocked in at 27 new lines of code which is quite a lot higher than my estimate
    of 6-12 lines. I know this task required some complicated state but I really didn't forsee the
    redundancy required to support both double and single quotes.


Copyright © 2023 Lars S.
