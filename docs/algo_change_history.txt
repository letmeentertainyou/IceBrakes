ALGORITHM IMPROVEMENT IDEAS!

These notes were living in TODO but it got a bit sloppy. I spent a while
writing down my thoughts and a little math for these changes so it would be
nice to keep those notes documented somewhere. Although I'm not really planning
on documenting the old code anywhere other than the git history.

Each one of these changes constitutes at least a 0.0.1 version number increase.
These changes will not effect any of the API and therefore any of the tests. So I 
will use pytest with our 10 existing tests to time out how much
these improvements speed up testing.

These changes are in linear order from the time of writing them so all test times carry
forward but there may be many more tests in the future.

*** 
BEFORE TIMES:
real    0m0.848s
user    0m0.747s
sys     0m0.102s


Modify IceBrakes algo to only read each file once. You still build two
dicts for CONSTANTS and ALL_VARS but you do it in one pass.

This will be uglier code in some ways with a bit more nesting and a bit less
DRY potential. 

Our bigO time improvement looks like this:  O(Nᴹ + Nᴹ⁺¹)  ->  O(Nᴹ⁺¹)
Where N is the length of the file and M is the amount of checks I do per line
and the +1 represents the single extra check for #$ at the end of each line.

I'm having real trouble parsing this math because it casts a pretty wide net.
For smaller numbers we are looking at close to 100% speed up or double speed. Hopefully
that works out for my tests but as the size of N and M grow this improvement matters less.

Either way because the tests are very small the act of reading the file may actually be a
huge time sink and I think that we might see some huge improvements in the tests. Which I'm
now realizing is a classic trap to make one think their program is faster than it is.

Still I want my tests to run faster so I can do more of everything else.

The API change will look like this:
function get_names_from_file() will not be called twice anymore, instead we will
both dicts in one pass. This means the boolean check goes and the nifty logic with
target='' goes too. Then this method can be called wherever the two dicts are needed.

This one change will actually DRY this up a fair amount.

This broken test 0011 for a minute for reasons documented in the test file. Now we test
the new behavior instead. Which is more thorough anyways.

AFTER TIMES:
real    0m0.875s
user    0m0.755s
sys     0m0.122s


It's worth noting that currently my tests are very very short files which has been 
practical to maintain even though it's not realistic. I should grab some existing large
open source python code, and add my silly marks to it and use it for benchmarking my
program instead of the unit tests. I should have longer tests too.

It's possible my current unit tests run at a pretty linear near O(1) already but that doesn't 
invalidate my improvements or my math. 

I added a new benchmark that is 16k lines long and declares #$ on every single line
giving us our maximum possible runtime. We are not currently concerned with accurately
linting this file but we do want to track algo time. Now even though I tried to make
this fairly slow it still only takes 5.7 seconds. But I think that will be enough to
mark any further improvements.

***

BEFORE TIMES:
real    0m7.806s
user    0m5.724s
sys     0m0.782s

Modify IceBrakes algo to ignore names that aren't in the constants
dict. This will skip a large amount of checks, and when more tests
are run things will be way faster. 

Use the names as keys and the indexes as values. This is what I should have done
the whole time and it will be a time improvement on it's own as there are less total dict keys
to loop over.

(TIME ABOVE CHANGES BEFORE CONTINUING)

Next; loop through the keys only and do all_vars.get(constants_key) to determine values.
This is clever because it removes an entire for loop and python's get method is very fast.

O(Nᴹ)  ->  O(N)

I will have to review my code closely to figure out the real max time. It's written as a nested for
loop wright now which is bad. I need to loop over every line of the file exactly 1 for the purpose
of writing an output results.


O(Nᴹ)  ->  O(N)
O(Nᴹ + Nᴹ⁺¹)  ->  O(Nᴹ⁺¹)

This change was implemented in a scratchpad and move over to icebrakes.py without too much
trouble. This testing suite has been invaluable so far.

AFTER TIMES:
real    0m4.427s
user    0m1.233s
sys     0m0.657s

Nice! We went from 5.7 to 1.2 seconds and scored a 4.76x speed improvement. Now that both
algo changes are done, I need to focus on the white space parsing.
